{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiscunhacsc/curso_dl_session01/blob/main/ISMT_DL_Session01_ExC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1pxQ95jI7b0"
      },
      "source": [
        "# Célula 1: Introdução\n",
        "Neste exemplo, vamos explorar como usar modelos pré-treinados para classificação de imagens. O conjunto de dados que utilizaremos é o CIFAR-100, que consiste em 60.000 imagens coloridas de 32x32 pixels, distribuídas por 100 classes, com 600 imagens por classe.\n",
        "\n",
        "Usaremos um modelo pré-treinado, faremos ajuste fino e utilizaremos técnicas como data augmentation e regularização.\n",
        "\n",
        "Vamos começar importando as bibliotecas necessárias."
      ],
      "id": "P1pxQ95jI7b0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxAxLBHfI7cE"
      },
      "source": [
        "# Célula 2: Importar bibliotecas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "id": "OxAxLBHfI7cE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ZqityiI7cL"
      },
      "source": [
        "# Célula 3: Carregar o conjunto de dados CIFAR-100\n",
        "Vamos carregar o conjunto de dados CIFAR-100 e fazer uma normalização das imagens."
      ],
      "id": "Z4ZqityiI7cL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6wb7VDGI7cM"
      },
      "source": [
        "# Célula 4: Carregar e normalizar os dados\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "train_labels = to_categorical(train_labels, 100)\n",
        "test_labels = to_categorical(test_labels, 100)"
      ],
      "id": "B6wb7VDGI7cM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MIboqmyI7cO"
      },
      "source": [
        "# Célula 5: Usando um modelo pré-treinado\n",
        "Vamos utilizar o modelo ResNet50 que foi pré-treinado no conjunto de dados ImageNet. Inicialmente, usaremos o modelo diretamente para fazer previsões."
      ],
      "id": "_MIboqmyI7cO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFwZH5hpI7cQ"
      },
      "source": [
        "# Célula 6: Utilizar o modelo pré-treinado\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# Congelar todas as camadas do modelo base\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Adicionar camadas personalizadas\n",
        "x = layers.Flatten()(base_model.output)\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "outputs = layers.Dense(100, activation='softmax')(x)\n",
        "\n",
        "# Construir o modelo\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "history_pretrained = model.fit(train_images, train_labels, batch_size=128, epochs=5, validation_data=(test_images, test_labels))"
      ],
      "id": "JFwZH5hpI7cQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNoM55JXI7cR"
      },
      "source": [
        "# Célula 7: Ajuste fino do modelo\n",
        "Agora, vamos descongelar algumas das camadas do modelo base e realizar ajuste fino."
      ],
      "id": "MNoM55JXI7cR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8Ytc-kFI7cR"
      },
      "source": [
        "# Célula 8: Descongelar camadas e realizar ajuste fino\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "history_fine_tuning = model.fit(train_images, train_labels, batch_size=128, epochs=5, validation_data=(test_images, test_labels))"
      ],
      "id": "i8Ytc-kFI7cR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIdjwjI1I7cS"
      },
      "source": [
        "# Célula 9: Data Augmentation\n",
        "Agora vamos utilizar a técnica de data augmentation para melhorar o desempenho do nosso modelo. Esta técnica consiste em gerar novas imagens a partir das originais, aplicando transformações como rotação, zoom, inversão, etc."
      ],
      "id": "TIdjwjI1I7cS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbgJnGR-I7cT"
      },
      "source": [
        "# Célula 10: Data Augmentation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "data_augmentation = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\")\n",
        "\n",
        "# Treinar o modelo com data augmentation\n",
        "history_with_augmentation = model.fit(data_augmentation.flow(train_images, train_labels, batch_size=128), epochs=5, validation_data=(test_images, test_labels))"
      ],
      "id": "sbgJnGR-I7cT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi0Y_J-UI7cU"
      },
      "source": [
        "# Célula 11: Avaliar e comparar o desempenho do modelo\n",
        "Vamos avaliar o desempenho dos modelos e comparar como eles se saíram com diferentes técnicas."
      ],
      "id": "Fi0Y_J-UI7cU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X82Q3IFwI7cU"
      },
      "source": [
        "# Célula 12: Plotar gráficos de desempenho\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_pretrained.history['val_accuracy'], label='Pré-treinado')\n",
        "plt.plot(history_fine_tuning.history['val_accuracy'], label='Ajuste fino')\n",
        "plt.plot(history_with_augmentation.history['val_accuracy'], label='Data Augmentation')\n",
        "plt.legend()\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Acurácia de Validação')\n",
        "plt.title('Desempenho do Modelo')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_pretrained.history['val_loss'], label='Pré-treinado')\n",
        "plt.plot(history_fine_tuning.history['val_loss'], label='Ajuste fino')\n",
        "plt.plot(history_with_augmentation.history['val_loss'], label='Data Augmentation')\n",
        "plt.legend()\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Perda de Validação')\n",
        "plt.title('Desempenho do Modelo')\n",
        "\n",
        "plt.show()"
      ],
      "id": "X82Q3IFwI7cU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up9B7_M4I7cV"
      },
      "source": [
        "# Célula 13: Conclusão\n",
        "Neste exemplo, explorámos como podemos utilizar modelos pré-treinados para classificação de imagens utilizando o conjunto de dados CIFAR-100.\n",
        "\n",
        "Começámos utilizando o modelo diretamente, depois fizemos ajuste fino e finalmente utilizámos data augmentation.\n",
        "\n",
        "É importante notar que escolher as melhores técnicas e hiperparâmetros pode depender do conjunto de dados e do problema específico. Experimentar e avaliar diferentes abordagens é uma parte importante do processo de desenvolvimento de modelos de deep learning."
      ],
      "id": "Up9B7_M4I7cV"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}